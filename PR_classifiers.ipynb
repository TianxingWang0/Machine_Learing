{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description:<br>\n",
    "Based on the MNIST dataset, design and implement classifiers including: <br>\n",
    "1) least squares with regularization<br>\n",
    "2) Fisher discriminant analysis (with kernels)<br>\n",
    "3) Perceptron (with kernels)<br>\n",
    "4) logistic regression<br>\n",
    "5) SVM (with kernels)<br>\n",
    "6) MLP-NN with two different error functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP-NN with two different error functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the input data to be suitable for network input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((60000, 28 * 28))\n",
    "X_train = X_train.astype('float32') / 255\n",
    "\n",
    "X_test = X_test.reshape((10000, 28 * 28))\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the neural network, the network contains two dense layers, they are fully connected. The first layer use activation function ```relu(x) = max(x, 0)```The second (and last) layer is a 10-way softmax layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network1 = models.Sequential()\n",
    "network1.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network1.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling network1, use loss function ```categorical_crossentropy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training network1, the network will start to iterate on the training data in mini-batches of 128 samples, 5 times over (each iteration over all the training data is called an epoch). At each iteration, the network will compute the gradients of the weights with regard to the loss on the batch, and update the weights accordingly. After these 5 epochs, the network will have performed 2,345 gradient updates (469 per epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.2581 - acc: 0.9259\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.1036 - acc: 0.9701\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.0684 - acc: 0.9796\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 92us/step - loss: 0.0501 - acc: 0.9847\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.0374 - acc: 0.9889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18131c405c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network1.fit(X_train, y_train, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct network2 as above, but use loss function ```categorical_hinge```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2283 - acc: 0.8973\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.0986 - acc: 0.9562\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.0704 - acc: 0.9686\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.0556 - acc: 0.9755\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.0461 - acc: 0.9794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1813366ce48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network2 = models.Sequential()\n",
    "network2.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network2.add(layers.Dense(10, activation='softmax'))\n",
    "network2.compile(optimizer='rmsprop', loss='categorical_hinge', metrics=['accuracy'])\n",
    "network2.fit(X_train, y_train, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the networks with testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 74us/step\n",
      "test_acc of network1:  0.9793\n",
      "10000/10000 [==============================] - 1s 72us/step\n",
      "test_acc of network2:  0.9735\n"
     ]
    }
   ],
   "source": [
    "test_loss1, test_acc1 = network1.evaluate(X_test, y_test)\n",
    "print('test_acc of network1: ', test_acc1)\n",
    "test_loss2, test_acc2 = network2.evaluate(X_test, y_test)\n",
    "print('test_acc of network2: ', test_acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (with kernels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%s of %s test values correct. (9857, 10000)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape((60000, 28 * 28))\n",
    "X_train = X_train.astype('float32') / 255\n",
    "\n",
    "X_test = X_test.reshape((10000, 28 * 28))\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC(C=100.0, kernel='rbf', gamma=0.03)\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svc = [int(a) for a in svc_model.predict(X_test)]\n",
    "correct = sum(int(x == y) for x, y in zip(y_pred_svc, y_test))\n",
    "\n",
    "print(\"%s of %s test values correct.\", (correct, len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with regularization (Ridge)###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\28347\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:40: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number/precision: 1.3002339205314684e-09 / 5.960464477539063e-08\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of alpha =  0.001  is  0.8602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\28347\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py:40: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number/precision: 1.3013183419730012e-08 / 5.960464477539063e-08\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of alpha =  0.01  is  0.8603\n",
      "The accuracy score of alpha =  0.1  is  0.8604\n",
      "The accuracy score of alpha =  0.5  is  0.8605\n",
      "The accuracy score of alpha =  1  is  0.8604\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for alpha in [0.001, 0.01, 0.1, 0.5, 1]:\n",
    "    ridge_model = RidgeClassifier(alpha=alpha)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    y_pred = ridge_model.predict(X_test)\n",
    "    print(\"The accuracy score of alpha = \", alpha, \" is \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, least squares with regularization (ridge) get a relatively higher accuracy score when ```alpha = 0.5```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:   44.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score is:  0.862\n",
      "Best params are:  {'penalty': 'l2', 'tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression()\n",
    "penalty_options = ['l1', 'l2']\n",
    "tol_options = [0.0001, 0.00001, 0.000001, 0.0000001]\n",
    "param_options = dict(penalty=penalty_options,tol=tol_options)\n",
    "grid = GridSearchCV(lr_model, param_options, cv=10, scoring='accuracy', verbose=1)\n",
    "X_train = X_train[:1000,:]\n",
    "y_train = y_train[:1000]\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best score is: ', str(grid.best_score_))\n",
    "print('Best params are: ', str(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron (with kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference artical is ```Jianhua Xu and Xuegong Zhang, \"A multiclass kernel perceptron algorithm,\" 2005 International Conference on Neural Networks and Brain, Beijing, 2005, pp. 717-721.doi: 10.1109/ICNNB.2005.1614728```. I implemented the kernel perceptron all over with python. But I realized if use kernel perceptron to handle MNIST, the model has to save whole training dataset to do prediction, and one single prediction is too costly to endure not to mention the fitting process (the magnitude is bigger than billion). So I choose to analyse the ```iris``` dataset to test my code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset ```iris```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My code of kernel perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KernelPerceptron:\n",
    "    '''\n",
    "    :parameter\n",
    "    kernel: the kernel function, default as rbf, can choose poly and linear otherwise\n",
    "    param: the param of sigma**2 (rbf) or the degree of polynomial, default as 3\n",
    "    '''\n",
    "\n",
    "    def __init__(self, kernel='rbf', param=3):\n",
    "        self.c_ = 0             # the c classes problem\n",
    "        self.alpha_ = np.zeros(1)\n",
    "        self.beta_ = np.zeros(1)\n",
    "        self.N_ = 0             # the number of training dataset\n",
    "        self.k_ = np.zeros(1)   # store the training dataset to predict new input\n",
    "        if kernel == 'linear':\n",
    "            self.method_ = self.__linearKernel\n",
    "        elif kernel == 'rbf':\n",
    "            self.param_ = param\n",
    "            self.method_ = self.__rbfKernel\n",
    "        else:\n",
    "            self.param_ = param\n",
    "            self.method_ = self.__polyKernel\n",
    "\n",
    "    def __linearKernel(self, a, b):\n",
    "        return np.dot(a, b)\n",
    "\n",
    "    def __polyKernel(self, a, b):\n",
    "        return np.dot(a, b) ** self.param_\n",
    "\n",
    "    def __rbfKernel(self, a, b):\n",
    "        return np.exp(- sum((a - b) ** 2) / (2 * self.param_))\n",
    "\n",
    "    def fit(self, X_train, y_train):    # input y_train data type is float 0,1,2...\n",
    "        labels = set()\n",
    "        for t in y_train:            # get the number of classes\n",
    "            if t not in labels:\n",
    "                labels.add(t)\n",
    "        self.N_ = len(X_train)\n",
    "        self.c_ = len(labels)        # The number of classes\n",
    "        self.alpha_ = np.zeros((self.c_, self.N_))\n",
    "        self.beta_ = np.zeros((self.c_, 1))\n",
    "\n",
    "        gram = np.zeros((self.N_, self.N_))      # calculate the gram matrix\n",
    "        for i in range(self.N_):\n",
    "            for j in range(i + 1):\n",
    "                gram[i][j] = self.method_(X_train[i], X_train[j])\n",
    "                if i != j:\n",
    "                    gram[j][i] = gram[i][j]\n",
    "\n",
    "        for i in range(self.N_):\n",
    "            x = X_train[i]          # exam sample x\n",
    "            ti = y_train[i]         # target value ti\n",
    "            y = np.dot(self.alpha_, gram[i][:, np.newaxis]) + self.beta_\n",
    "            j = np.where(y == max(y))[0][0]             # the index of the biggest\n",
    "            if j != ti:                                 # misclassified sample\n",
    "                for m in range(self.N_):\n",
    "                    self.alpha_[ti][m] += np.exp(- sum((X_train[m] - x) ** 2) / 2)\n",
    "                    self.alpha_[j][m] -= np.exp(- sum((X_train[m] - x) ** 2) / 2)\n",
    "                self.beta_[ti] += 1\n",
    "                self.beta_[j] -= 1\n",
    "\n",
    "        # The training has been done, now store X_train for the future prediction\n",
    "        self.k_ = X_train\n",
    "\n",
    "    def __y(self, x):\n",
    "        kx = np.zeros(self.N_)\n",
    "        for i in range(self.N_):\n",
    "            kx[i] = self.method_(self.k_[i], x)\n",
    "        kx = kx[:, np.newaxis]\n",
    "        y = np.dot(self.alpha_, kx) + self.beta_\n",
    "        return np.where(y == max(y))[0][0]\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        total = len(X_test)\n",
    "        y_pred = np.zeros(total)\n",
    "        for i in range(total):\n",
    "            y_pred[i] = self.__y(X_test[i])\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test my code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of my kernel perceptron is  0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "kp_model = KernelPerceptron(kernel='rbf', param=1)\n",
    "kp_model.fit(X_train, y_train)\n",
    "y_pred_kp = kp_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('The accuracy score of my kernel perceptron is ', accuracy_score(y_test, y_pred_kp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher discriminant analysis (with kernels) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I inplemented the algorithm mentioned in ```https://en.wikipedia.org/wiki/Kernel_Fisher_discriminant_analysis#cite_note-texture-7```The kernel fisher solution confronts the same problem as kernel perceptron does. So I will test my code using ```iris```dataset instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My code of kernel fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eigh as largest_eigh\n",
    "\n",
    "class KernelFisher:\n",
    "    def __init__(self, kernel='rbf', param=3):\n",
    "        self.yMeans_ = []           # yMeans[i] is the projected mean for class i\n",
    "        self.X_ = []                # store the training dataset for prediction\n",
    "        self.AstarT_ = np.mat(1)    # The A*T used in ultimate decision function\n",
    "        if kernel == 'linear':\n",
    "            self.method_ = self.__linearKernel\n",
    "        elif kernel == 'rbf':\n",
    "            self.param_ = param\n",
    "            self.method_ = self.__rbfKernel\n",
    "        else:\n",
    "            self.param_ = param\n",
    "            self.method_ = self.__polyKernel\n",
    "\n",
    "    def __linearKernel(self, a, b):\n",
    "        return np.dot(a, b)\n",
    "\n",
    "    def __polyKernel(self, a, b):\n",
    "        return np.dot(a, b) ** self.param_\n",
    "\n",
    "    def __rbfKernel(self, a, b):\n",
    "        return np.exp(- sum((a - b) ** 2) / (2 * self.param_))\n",
    "\n",
    "    def __y(self, xt):\n",
    "        yt = []\n",
    "        for xi in self.X_:\n",
    "            yt.append(self.method_(xi, xt))\n",
    "        return np.array(self.AstarT_ * np.mat(yt).T)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_ = X_train\n",
    "        n = len(X_train)                  # size of training dataset\n",
    "        c = int(max(y_train)) + 1         # y_train must be coded as 0,1,2...\n",
    "        Ms = []                           # Ms[i] = Mi\n",
    "        Ks = []                           # Ks[j] = Kj\n",
    "        sortedByClass = []         # sortedByClass[i] is a np.array contains all points belong to class i\n",
    "\n",
    "        for y in range(c):\n",
    "            currentClass = []\n",
    "            for i in range(n):\n",
    "                if y_train[i] == y:\n",
    "                    currentClass.append(X_train[i])\n",
    "            sortedByClass.append(np.array(currentClass))\n",
    "\n",
    "        for j in range(c):\n",
    "            kj = []\n",
    "            for xi in X_train:\n",
    "                rowi = []\n",
    "                for xj in sortedByClass[j]:\n",
    "                    rowi.append(self.method_(xi, xj))\n",
    "                kj.append(np.array(rowi))\n",
    "            Ks.append(np.array(kj))\n",
    "\n",
    "        for i in range(c):\n",
    "            Mi = []\n",
    "            for xi in X_train:\n",
    "                sum = 0\n",
    "                for xj in sortedByClass[i]:\n",
    "                    sum += self.method_(xi, xj)\n",
    "                Mi.append(sum)\n",
    "            Ms.append(np.array(Mi)[:, np.newaxis] / len(sortedByClass[i]))\n",
    "\n",
    "        Mstar = []\n",
    "\n",
    "        for xi in X_train:\n",
    "            sum = 0\n",
    "            for xj in X_train:\n",
    "                sum += self.method_(xi, xj)\n",
    "            Mstar.append(sum)\n",
    "\n",
    "        Mstar = np.array(Mstar)[:, np.newaxis] / n\n",
    "        M = np.zeros((n, n))\n",
    "\n",
    "        for i in range(c):\n",
    "            M += (Ms[i] - Mstar) * (Ms[i] - Mstar).T / len(sortedByClass[i])\n",
    "\n",
    "        reversedN = np.zeros((n, n))            # the reversed matrix N\n",
    "\n",
    "        for j in range(c):\n",
    "            Nj = len(sortedByClass[j])\n",
    "            reversedN += np.dot(np.dot(Ks[j], (np.eye(Nj) - np.full((Nj, Nj), 1 / Nj))), Ks[j].T)\n",
    "\n",
    "        reversedN = np.mat(reversedN).I\n",
    "\n",
    "        evals, self.AstarT_ = largest_eigh(reversedN * M, eigvals=(n - c + 1, n - 1))\n",
    "        self.AstarT_ = self.AstarT_.T\n",
    "\n",
    "        for j in range(c):\n",
    "            m = 0\n",
    "            for xi in sortedByClass[j]:\n",
    "                m += self.__y(xi)\n",
    "            m /= len(sortedByClass[j])\n",
    "            self.yMeans_.append(np.array(m))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for xt in X_test:\n",
    "            dis = []\n",
    "            yt = self.__y(xt)\n",
    "            for mj in self.yMeans_:\n",
    "                dis.append(np.sum(np.abs(yt - mj)))\n",
    "            y_pred.append(dis.index(min(dis)))\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test my code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "kf_model = KernelFisher(kernel='rbf', param=1)\n",
    "kf_model.fit(X_train, y_train)\n",
    "y_pred_kf = kf_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred_kf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
